# -*- coding: utf-8 -*-
"""
Generate spike trains which vary with firing rate taken from the same distribution
to mimic a patterned neural response.

A baseline level of noise can be varied to add non-patterned spikes to the response

Reproducibility is calculated using the same method as Brette 2012, PLoS Comp Bio
(code copied from Fig5E_precision_reliability, received from Romain Brette)


"""
#%% some importing
import brian2 as br
import scipy.stats as sp
import random as rd
import numpy as np
import itertools

# Code received from Romain Brette
def autocor(PSTH,N=None,T=20*br.ms,bin=None):
    '''
    Autocorrelogram of PSTH, to calculate a shuffled autocorrelogram
    
    N = number of spike trains
    T = temporal window
    bin = PSTH bin
    The baseline is not subtracted.
    
    Returns times,SAC
    '''
    if bin is None:
        bin = br.defaultclock.dt

    p = int(T/ bin)
    SAC = br.zeros(p)
    
    if N is None:
        SAC[0] = br.mean(PSTH * PSTH)
    else: # correction to exclude self-coincidences
        PSTHnoself = br.clip(PSTH - 1. / (bin * N), 0*br.kHz, br.Inf*br.kHz)
        SAC[0] = br.mean(PSTH * PSTHnoself) * N / (N - 1.)
    SAC[1:] = [br.mean(PSTH[:-i] * PSTH[i:]) for i in range(1,p)]
    SAC = br.hstack((SAC[::-1], SAC[1:]))
    out = (br.arange(len(SAC)) - len(SAC) / 2) * bin
    
    return out, SAC

#%% 
'''
This is where the pattern distributions are generated and random spikes are
selected. At the end the reproducibility is calculated for each combination
of variables.

We are varying the coincidence window (cw) which is the bin size of for the
SAC as described by Philip Joris, the baseline level of noise, and the mean
number of spikes 
'''

# These are variables that being changed to check the effect on reproducibility
cw_list = (0.000001, 0.000005, 0.000010, 0.000050, 0.000100, 0.000500, 0.001000)
noise_list = np.arange(1, 21, 2)
n_spikes_list = np.arange(1, 21, 2)

# Output variables
results = []
key = []
curve = []
times_spikes = []
times_y = []

# Some constants
pattern = (0.002, 0.01, 0.05, 0.12, 0.2, 0.24, 0.5, 0.61, 0.69, 0.73, 0.8, 0.91)
var = 0.005
dur = 1
spikebin = 0.000050
spikebins = np.arange(0, dur + spikebin, spikebin)
reps = 100
mw_time = 0.02

# Now run through all combinations of the variables
for cw, noise, n_spikes in itertools.product(cw_list, noise_list, n_spikes_list):
    
    print(cw, noise, n_spikes)
    key.append((cw,noise,n_spikes))
    
    maxWidth = int(round(mw_time / cw))
    
    distri = np.zeros(len(spikebins))
    for p in pattern:
        distri = distri + sp.norm.pdf(spikebins, loc = p, scale = var)
        
    noise_prob = np.ones(len(spikebins), dtype=int) * noise
    distri = (distri + noise_prob) / (sum(distri) + sum(noise_prob))
    distri = np.append(np.array(0), np.cumsum(distri))
    distri[-1] += 0.00001
    
    spike = np.zeros((reps, n_spikes))
    for r, n in itertools.product(range(reps), range(n_spikes)):
        idx = int(np.digitize(rd.random(), distri))
        spike[r,n] = spikebins[idx-1] * 1000
        
    y = np.squeeze([np.ones(n_spikes)*r for r in range(reps)])
        
    PSTH, bin_edges = np.histogram(spike, bins = np.arange(0, dur + cw, cw) * 1000)
    PSTH = PSTH * br.kHz
    try:
        t, SAC = autocor(PSTH, reps, T=30*br.ms)
        SAC = SAC * br.Hz ** 2
        strength = (sum(SAC - (np.mean(PSTH) ** 2)) * (cw * br.second) / np.mean(PSTH) )
        SACcurve = ((SAC - np.mean(PSTH) ** 2) * (cw * br.second) / np.mean(PSTH) )
    except:
        strength = None
        SACcurve = None
        pass
    
    spike = np.reshape(spike, -1)
    y = np.reshape(y, -1)
    
    times_spikes.append([spike])
    times_y.append([y])
    results.append(strength)
    curve.append(SACcurve)

np.savez('testing_results', 
         results = results, 
         key = key, 
         curve = curve,
         times_spikes = times_spikes,
         times_y = times_y)