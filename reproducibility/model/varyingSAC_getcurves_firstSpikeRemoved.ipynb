{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T21:08:25.330870Z",
     "start_time": "2020-01-30T21:08:25.298955Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#%% importing and defining functions\n",
    "\n",
    "#### import some stuff\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from os import chdir, listdir\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import brian2 as br\n",
    "from pandas import Series, DataFrame\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# % % define some useful functions\n",
    "\n",
    "def flatten_array(data):\n",
    "    d = data[0][0].flatten()\n",
    "    for i in np.arange(0, len(d)):\n",
    "        if d[i] is not np.empty:\n",
    "            d[i] = d[i][0]\n",
    "        else:\n",
    "            d[i] = np.empty(1)   \n",
    "    out = np.concatenate(([i for i in d]))\n",
    "    return out\n",
    "\n",
    "def autocor(PSTH,N=None,T=20*br.ms,bin=None):\n",
    "    if bin is None:\n",
    "        bin = br.defaultclock.dt\n",
    "    p = int(T/ bin)\n",
    "    SAC = br.zeros(p)    \n",
    "    if N is None:\n",
    "        SAC[0] = br.mean(PSTH * PSTH)\n",
    "    else: # correction to exclude self-coincidences\n",
    "        PSTHnoself = br.clip(PSTH - 1. / (bin * N), 0*br.kHz, br.Inf*br.kHz)\n",
    "        SAC[0] = br.mean(PSTH * PSTHnoself) * N / (N - 1.)\n",
    "    SAC[1:] = [br.mean(PSTH[:-i] * PSTH[i:]) for i in range(1,p)]\n",
    "    SAC = br.hstack((SAC[::-1], SAC[1:]))\n",
    "    out = (br.arange(len(SAC)) - len(SAC) / 2) * bin    \n",
    "    return out, SAC\n",
    "\n",
    "def make_PSTH(spikes_flat, dur, cw, reps, start):\n",
    "    PSTH, bin_edges = np.histogram(spikes_flat, bins = np.arange(start, dur + 0.05 * br.second + cw, cw))\n",
    "    PSTH = PSTH / (cw * reps)\n",
    "    return PSTH\n",
    "\n",
    "def jorisnorm(sac, N, m, dur, cw):\n",
    "    nf = N * (N - 1) * m**2 * dur * cw * br.hertz ** 2\n",
    "    sacnorm = sac / nf\n",
    "    strength = np.mean(sacnorm)\n",
    "    return sacnorm, strength\n",
    "\n",
    "def brettenorm(sac, m, cw):\n",
    "    sacnorm = (sac - m ** 2) * cw / m\n",
    "    strength = sum(sac - m ** 2) * cw / m\n",
    "    return sacnorm, strength\n",
    "\n",
    "def flatten(l):\n",
    "  out = []\n",
    "  for item in l:\n",
    "    if isinstance(item, (list, tuple)):\n",
    "      out.extend(flatten(item))\n",
    "    else:\n",
    "      out.append(item)\n",
    "  return out\n",
    "\n",
    "def convert_ms2sec( mat ):\n",
    "    for r, c in np.ndindex(mat.shape): \n",
    "        mat[r][c] = mat[r][c] / 1000\n",
    "    return mat\n",
    "\n",
    "# % % Load in data from MatLab files\n",
    "  # using functions to ease the amount of necessary commenting\n",
    "    \n",
    "# ICls ITD\n",
    "def load_icls_itd():\n",
    "    dur = 0.25\n",
    "    name = 'itd'\n",
    "    chdir('K:\\Data\\ICls\\Data\\RawData\\Dichotic\\ITD')\n",
    "    path= 'K:\\Data\\ICls\\Data\\RawData\\Dichotic\\ITD'\n",
    "    \n",
    "    TrialData = []\n",
    "    for f in sorted(listdir(path)):\n",
    "        data = sio.loadmat(f)\n",
    "        curvedata = data['curvedata']\n",
    "        TrialData.append(curvedata['spike_times'][0][0])\n",
    "        del data, curvedata\n",
    "    TrialData = map(convert_ms2sec, TrialData)\n",
    "    files = sorted(listdir(path))\n",
    "    return TrialData, dur, name, files\n",
    "\n",
    "# ICls ILD\n",
    "def load_icls_ild():    \n",
    "    dur = 0.25\n",
    "    name = 'ild'\n",
    "    chdir('K:\\Data\\ICls\\Data\\RawData\\Dichotic\\ILD')\n",
    "    path= 'K:\\Data\\ICls\\Data\\RawData\\Dichotic\\ILD'\n",
    "    \n",
    "    TrialData = []\n",
    "    for f in sorted(listdir(path)):\n",
    "        data = sio.loadmat(f)\n",
    "        curvedata = data['curvedata']\n",
    "        TrialData.append(curvedata['spike_times'][0][0])\n",
    "        del data, curvedata\n",
    "    TrialData = map(convert_ms2sec, TrialData)\n",
    "    files = sorted(listdir(path))\n",
    "    return TrialData, dur, name, files\n",
    "\n",
    "# ICls FF\n",
    "def load_icls_ff():\n",
    "    dur = 0.45\n",
    "    name = 'ff'\n",
    "    chdir('K:\\Data\\ICls\\Data\\RawData\\FF')\n",
    "    path = 'K:\\Data\\ICls\\Data\\RawData\\FF'\n",
    "    \n",
    "    TrialData = []\n",
    "    for f in sorted(listdir(path)):\n",
    "        data = sio.loadmat(f)\n",
    "        TrialData.append(data['TrialData'])\n",
    "        del data\n",
    "    TrialData = map(convert_ms2sec, TrialData)\n",
    "    files = sorted(listdir(path))\n",
    "    return TrialData, dur, name, files\n",
    "\n",
    "# OT FF\n",
    "def load_ot_ff():\n",
    "    dur = 0.15\n",
    "    name = 'ot'\n",
    "    data = sio.loadmat('K:\\WorkingFolder\\distance\\data\\FreeField_OT_python_firstSpikeRemoved.mat')\n",
    "    data = data['data1']\n",
    "    TrialData = []\n",
    "    files = []\n",
    "    for f, path in enumerate(data):\n",
    "        tmp = data[f][0]\n",
    "        for ix, tt in enumerate(tmp):\n",
    "            tmp[ix] = [np.reshape(np.concatenate(t), [1, len(t)])  if np.size(t) > 0 else t for t in tt]\n",
    "        TrialData.append(tmp)\n",
    "        files.append(f)\n",
    "    return TrialData, dur, name, files\n",
    "\n",
    "# Get the integral of SAC peak out\n",
    "def peak_integral(SAC, int_w):\n",
    "    repro = np.sum(SAC[(len(SAC)/2 + 1) - int_w:(len(SAC)/2 + 1) + int_w])\n",
    "    return repro    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T21:08:28.640979Z",
     "start_time": "2020-01-30T21:08:28.019013Z"
    }
   },
   "outputs": [],
   "source": [
    "#%% Load data \n",
    "\n",
    "# Here execute the desired data set\n",
    "#TrialData, dur, name, files = load_icls_itd()\n",
    "#TrialData, dur, name, files = load_icls_ild()\n",
    "#TrialData, dur, name, files = load_icls_ff()\n",
    "TrialData, dur, name, files = load_ot_ff()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T21:08:32.856882Z",
     "start_time": "2020-01-30T21:08:31.335613Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: RuntimeWarning: invalid value encountered in double_scalars\n",
      " [py.warnings]\n"
     ]
    }
   ],
   "source": [
    "#%% Actual Work\n",
    "##### make histograms for each data file\n",
    "\n",
    "chdir('K:\\WorkingFolder')\n",
    "\n",
    "note = '_100micro'\n",
    "\n",
    "# some variables\n",
    "spikebin = 0.001 * br.second\n",
    "\n",
    "hist = []\n",
    "full_fr = []\n",
    "x_spikes = []\n",
    "y_spikes = []\n",
    "\n",
    "for idx, t in enumerate(TrialData):\n",
    "    hist.append([])\n",
    "    for xx, da in enumerate(t):\n",
    "        reps = len(da)\n",
    "        y = [np.ones(np.size(s, 1)) * (tt + 1) for tt, s in enumerate(da) if s.size != 0]\n",
    "        check_dim = map(lambda x: x == 0, map(np.size, da))\n",
    "        check_dim = [c for c, chck in enumerate(check_dim) if chck]\n",
    "        da = np.delete(da, check_dim)\n",
    "        da = [s[0] for s in da]\n",
    "        a = [np.shape(r) for r in da]\n",
    "        full_fr.append(np.sum([b[0] for b in a]) / float(reps) / (dur - 0.05))\n",
    "        try:\n",
    "            x_spikes.append(np.concatenate(da))\n",
    "        except:\n",
    "            x_spikes.append(da)\n",
    "        try:\n",
    "            y_spikes.append(np.concatenate(y))  \n",
    "        except:\n",
    "            y_spikes.append(y)              \n",
    "        try:\n",
    "            hist[idx].append(make_PSTH(np.concatenate(da), dur * br.second, spikebin, reps, 0.1 * br.second))\n",
    "        except:\n",
    "            hist[idx].append(None)\n",
    "\n",
    "cw = spikebin / br.second\n",
    "del spikebin, idx, da, check_dim, TrialData, a, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T21:09:19.567464Z",
     "start_time": "2020-01-30T21:08:40.320129Z"
    }
   },
   "outputs": [],
   "source": [
    "# % % run reproducibility\n",
    "\n",
    "# variables\n",
    "curve_time = 100\n",
    "\n",
    "repro = DataFrame(columns = ['neuron', 'depvar', 'fr', 'curve'])\n",
    "\n",
    "for neu, h in enumerate(hist):\n",
    "    \n",
    "    for dv, p in enumerate(h):\n",
    "        try:\n",
    "            t, SAC = autocor(p, reps, T=curve_time*br.ms, bin=cw * br.second)\n",
    "            SAC = SAC * br.Hz ** 2\n",
    "            sp = np.mean(p)\n",
    "        except:\n",
    "            strength_b = np.nan\n",
    "            arr = np.arange(-(curve_time * br.ms), (curve_time * br.ms) , cw * br.second)\n",
    "            arr = np.delete(arr, 0)\n",
    "            arr[:] = np.nan\n",
    "            SAC = arr\n",
    "            sp = np.nan\n",
    "            pass\n",
    "        tmp = {'neuron': neu, 'depvar': dv, 'fr':sp, 'curve': SAC}\n",
    "        repro = repro.append(tmp, ignore_index = True)\n",
    "    \n",
    "repro['fr'] = repro['fr'] / br.Hz\n",
    "repro['x_spikes'] = x_spikes\n",
    "repro['y_spikes'] = y_spikes\n",
    "repro['full_fr'] = full_fr\n",
    "\n",
    "#sio.savemat('repro_ff.mat', {'fr':fr, 'r_brette':r_brette, 'r_joris':r_joris})\n",
    "\n",
    "curve = [c for c in repro['curve']]\n",
    "sio.savemat('repro_' + name + '_curve' + note + 'firstSpikeRemoved.mat', {'curve':curve, 'cw':cw, 'neuron':repro['neuron'].tolist(), 'depvar':repro['depvar'].tolist(), 'fr':repro['fr'].tolist(), 'full_fr':full_fr, 'reps':reps, 'dur':dur})\n",
    "\n",
    "#df = repro.drop(columns = ['curve'])\n",
    "#export_csv =df.to_csv (r'f:\\desktop\\WorkingFolder\\repro_' + name + '_' + note + '.csv', index = None, header = True)\n",
    "\n",
    "del cw, tmp, neu, dv, h, p, t, sp, SAC, dur, hist, reps, x_spikes, y_spikes, full_fr\n",
    "\n",
    "files = pd.DataFrame({'neuron':range(len(files)), 'file':files})\n",
    "repro = pd.merge(files, repro)\n",
    "pickle.dump(repro, open('repro_' + name + note + '.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
